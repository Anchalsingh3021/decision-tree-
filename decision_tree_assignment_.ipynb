{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DECISION TREE ASSIGNMENT\n",
        "\n",
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "  - A decision tree is a type of machine learning model that is used for classification and prediction. It works like a flowchart, where each internal node represents a condition or test on a feature, branches represent possible outcomes, and leaf nodes show the final decision or class label. The model keeps splitting data based on the most important features, usually chosen by measures like Gini Index or Information Gain. In classification, the tree assigns new data to a class by following the path of conditions until it reaches a leaf. It’s simple to understand and visualize, making it popular for explaining decisions.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "\n",
        "  - Gini Impurity and Entropy are two ways to measure how mixed or “impure” a dataset is when building a decision tree. Gini Impurity shows the chance of misclassifying a randomly chosen element if it were labeled based on the distribution of classes. Entropy, on the other hand, comes from information theory and measures the amount of uncertainty or disorder in the data. When splitting, the decision tree looks for splits that reduce impurity the most, meaning the resulting groups are more pure and consistent. Lower impurity after a split means the decision tree is making better, more accurate classifications.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "  - Pre-pruning and post-pruning are techniques used to avoid overfitting in decision trees. **Pre-pruning** stops the tree from growing too deep by setting limits like maximum depth, minimum samples per split, or minimum leaf size. This saves time and keeps the model simpler. **Post-pruning**, on the other hand, first allows the tree to grow fully and then trims back branches that don’t add much predictive power. A key advantage of pre-pruning is faster training since the tree is controlled from the start, while post-pruning often gives better accuracy because it evaluates the full tree before simplifying it.\n",
        "\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "  - Information Gain tells us how much “clarity” we get after splitting the data using a feature. A feature with high information gain means it separates the data better, making the groups more pure. It is important because the decision tree always wants to choose the split that gives the most useful separation, leading to more accurate results.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "  - Decision trees are used in many real-life areas. For example, banks use them to check if someone is likely to repay a loan, doctors use them to help in diagnosing diseases, and businesses use them to predict customer behavior. Their main advantages are that they are easy to understand, explain, and visualize, even for non-technical people. However, they also have limitations—trees can easily become too complex and overfit the data, and small changes in data can sometimes change the whole tree structure.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NDZs9XRpDoh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcXacIQM_Onu",
        "outputId": "6f281cfa-e736-47bf-a77b-d58f3e3258b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ],
      "source": [
        "\"\"\"6. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\"\"\"\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train a Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\"\"\"\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree with max_depth=3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train a fully-grown Decision Tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# 6. Print and compare accuracies\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Decision Tree (max_depth=3) Accuracy:\", accuracy_depth3)\n",
        "print(\"Decision Tree (Fully-grown) Accuracy:\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_S8GZXyK4qt",
        "outputId": "40071b77-51d0-44cc-f11f-46cfd5f8b520"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree (max_depth=3) Accuracy: 1.0\n",
            "Decision Tree (Fully-grown) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\"\"\"\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# 2. Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, reg.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7uG8hFbLQ51",
        "outputId": "8068b965-c25a-4afe-9fad-a4c56407b7c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\"\"\"\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the Decision Tree and parameter grid\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# 4. Perform GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Get the best parameters and make predictions\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 6. Print results\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI1b8kzaLlOp",
        "outputId": "03d1895d-2da7-400a-a048-b65499266d65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a more humanized version of your answer in simple, natural language:\n",
        "\n",
        "  - If I were working as a data scientist for a healthcare company and needed to predict whether a patient has a certain disease, here’s how I would approach it:\n",
        "\n",
        "     **1. Handle missing values:** I would first look at the dataset to see which columns have missing information. For numbers like age or blood pressure, I’d fill the gaps with the average or median values. For categories like gender or blood type, I’d either use the most common value or create a separate “Unknown” category.\n",
        "\n",
        "     **2. Encode categorical features:** Since a decision tree works with numbers, I’d convert categorical features into numerical form. For example, unordered categories like blood type can use one-hot encoding, and ordered categories like disease stages can use label encoding.\n",
        "\n",
        "     **3. Train a Decision Tree model:** I’d split the data into training and testing sets and train a decision tree classifier. The tree would learn patterns in the data to separate patients with the disease from those without.\n",
        "\n",
        "     **4. Tune hyperparameters:** To make sure the model isn’t too simple or too complicated, I’d test different settings like the tree’s maximum depth or minimum samples per split using GridSearchCV or RandomizedSearchCV. This helps the model perform well on new, unseen patients.\n",
        "\n",
        "     **5. Evaluate performance:** I’d check the model’s accuracy, precision, recall, F1-score, and maybe ROC-AUC. In healthcare, it’s especially important to correctly identify sick patients, so I’d pay attention to false negatives.\n",
        "\n",
        "     **Business value:** This model could help doctors catch diseases earlier, prioritize patients who need urgent care, and make better treatment decisions. It could also save costs by avoiding unnecessary tests for low-risk patients while improving overall patient outcomes."
      ],
      "metadata": {
        "id": "Vxm4FRM2MnTU"
      }
    }
  ]
}